{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fashion dataset\n",
    "# DenseNet Application of Image Classification\n",
    "## Multi label inference\n",
    "The data is from the kaggle competition [iMaterialist Challenge (Fashion) at FGVC5](https://www.kaggle.com/c/imaterialist-challenge-fashion-2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.densenet import densenet121 as feature_extractor\n",
    "from torch import nn\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "SCALE = 256\n",
    "TRAIN = \"/data/fashion/img/train/\"\n",
    "VALID = \"/data/fashion/img/valid/\"\n",
    "TEST = \"/data/fashion/img/test/\"\n",
    "CUDA = torch.cuda.is_available()\n",
    "DENSE_FEATURE = 1024\n",
    "BS = 32\n",
    "# VERSION = \"0.0.1\" # bias =True for last linear\n",
    "VERSION = \"0.0.2\" # densenet 121\n",
    "# VERSION = \"0.0.3\" # densenet169\n",
    "CATE_LEN = 228\n",
    "\n",
    "# Mean and standard for normalization\n",
    "MEAN = [0.485, 0.456, 0.406]\n",
    "STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "BENCH_MARK = .5\n",
    "# CUDA = False\n",
    "print(CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torchvision/models/densenet.py:212: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  nn.init.kaiming_normal(m.weight.data)\n"
     ]
    }
   ],
   "source": [
    "conv_model = feature_extractor(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split ConvLayer to 2 parts, conv0~transtition3, (not gonna train), denseblock4~norm5 (train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_conv1 = nn.Sequential(*[getattr(conv_model.features,nn_name) for nn_name in [\"conv0\",\"norm0\",\"relu0\",\"pool0\",\"denseblock1\",\"transition1\",\n",
    "                                                                                   \"denseblock2\",\"transition2\",\"denseblock3\",\"transition3\",]])\n",
    "\n",
    "dense_conv2 = nn.Sequential(*[getattr(conv_model.features,nn_name) for nn_name in [\"denseblock4\",\"norm5\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torch.nn import functional as F\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No augmentation here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "                                transforms.Resize((SCALE,SCALE)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(MEAN,STD),\n",
    "                               ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specific data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fashion_data(Dataset):\n",
    "    def __init__(self,img_folder,cate_len,transform):\n",
    "        super(fashion_data,self).__init__()\n",
    "        self.img_folder = img_folder\n",
    "        self.fnames = os.listdir(self.img_folder)\n",
    "        self.cate_len = cate_len\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.fnames)\n",
    "    \n",
    "    def get_cate(self,url):\n",
    "        zr = torch.zeros(228)\n",
    "        zr[torch.LongTensor(list(int(i[1:])-1 for i in str(url).split(\".\")[0].split(\"_\")[1:]))]=1\n",
    "        return zr\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        img = Image.open(self.img_folder+self.fnames[idx]).convert(\"RGB\")\n",
    "        img = self.transform(img)\n",
    "        return img, self.get_cate(self.fnames[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class test_data(Dataset):\n",
    "    def __init__(self, img_folder, transform):\n",
    "        self.img_folder = img_folder\n",
    "        self.fnames = os.listdir(self.img_folder)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.fnames)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        img = Image.open(self.img_folder+self.fnames[idx]).convert(\"RGB\")\n",
    "        return self.transform(img),self.fnames[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = test_data(\"/data/fashion/img/test/\",transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(ds,batch_size=BS,shuffle=False)\n",
    "test_gen = iter(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs,iid = next(test_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train /Valid dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top half of the model, with fully connected layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Flatten,self).__init__()\n",
    "    def forward(self,x):\n",
    "        bs = x.size()[0]\n",
    "        return x.view(bs,-1)\n",
    "    \n",
    "def convlayer(f_in,f_out,ks,stride=1):\n",
    "    return nn.Sequential(*[\n",
    "        nn.Conv2d(f_in, f_out, ks, stride = stride, padding = ks//2,bias = True),\n",
    "        nn.BatchNorm2d(f_out),\n",
    "        nn.LeakyReLU(inplace = True),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n"
     ]
    }
   ],
   "source": [
    "fl = Flatten()\n",
    "FEATURE_WIDTH = dense_conv2(dense_conv1(torch.rand(2,3,SCALE,SCALE))).size()[1]\n",
    "print(FEATURE_WIDTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class top_half(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(top_half,self).__init__()\n",
    "        self.top_ = nn.Sequential(*[\n",
    "                                    convlayer(FEATURE_WIDTH,FEATURE_WIDTH//2,3,2),\n",
    "                                    convlayer(FEATURE_WIDTH//2,FEATURE_WIDTH//4,3,2),\n",
    "                                    Flatten(),\n",
    "                                    nn.Linear(FEATURE_WIDTH,DENSE_FEATURE,bias=False),\n",
    "                                    nn.BatchNorm1d(DENSE_FEATURE),\n",
    "                                    nn.LeakyReLU(inplace=True),\n",
    "                                    nn.Dropout(p=.5),\n",
    "                                    nn.Linear(DENSE_FEATURE,CATE_LEN,bias=False),\n",
    "                                   ])\n",
    "    def forward(self,x):\n",
    "        return self.top_(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Model, optimizer,train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_half_ = top_half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CUDA:\n",
    "    torch.cuda.empty_cache()\n",
    "    top_half_.cuda()\n",
    "    dense_conv1.cuda()\n",
    "    dense_conv2.cuda()\n",
    "\n",
    "# loss_func = nn.CrossEntropyLoss()\n",
    "# loss_func = nn.BCEWithLogitsLoss() # Binary cross entropy with logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from p3self.matchbox import Trainer,argmax,accuracy,supermean,f1_score,save_model,load_model\n",
    "\n",
    "def save_():\n",
    "    save_model(dense_conv2,\"/data/weights/dense_conv2.%s.pkl\"%(VERSION))\n",
    "    save_model(top_half_,\"/data/weights/top_half.%s.pkl\"%(VERSION))\n",
    "    \n",
    "def load_():\n",
    "    load_model(dense_conv2,\"/data/weights/dense_conv2.%s.pkl\"%(VERSION))\n",
    "    load_model(top_half_,\"/data/weights/top_half.%s.pkl\"%(VERSION))\n",
    "\n",
    "def with_n(x):\n",
    "    return \"\\n%s,\"%(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX = torch.range(1,CATE_LEN).repeat([BS,1]).long()\n",
    "LABEL_HEAD = torch.ByteTensor(np.ones((BS,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(ds,batch_size = BS, print_on = 5, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what happened on each step of training\n",
    "def action(*args,**kwargs):\n",
    "    x,y = args[0]\n",
    "    if CUDA:\n",
    "        x,y = x.cuda(),y.cuda()\n",
    "    x = x[:,:3,...]\n",
    "    y_ = top_half_(dense_conv2(dense_conv1(x)))\n",
    "    \n",
    "    accuracy,recall,precision,f1 = f1_score(y_,y)\n",
    "\n",
    "    return {\n",
    "        \"acc\":accuracy.item(),\n",
    "        \"recall\":recall.item(),\n",
    "        \"precision\":precision.item(),\n",
    "        \"f1\":f1.item(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"/data/fashion/upload_%s.csv\"%(VERSION)\n",
    "os.system(\"echo 'image_id,label_id' > %s\"%(fname))\n",
    "def test_action(*args,**kwargs):\n",
    "    x,iid = args[0]\n",
    "    if CUDA:\n",
    "        x = x.cuda()\n",
    "    x = x[:,:3,...]\n",
    "    y_ = top_half_(dense_conv2(dense_conv1(x)))\n",
    "    \n",
    "    iid_long = torch.LongTensor(list(int(i) for i in iid)).unsqueeze(-1)\n",
    "    mark = torch.cat([iid_long,IDX[:len(iid)]],1).numpy().astype(str)\n",
    "    mark[...,0]=np.vectorize(with_n)(mark[...,0])\n",
    "\n",
    "    if CUDA:\n",
    "        y_=y_.cpu()\n",
    "\n",
    "    slic = torch.cat([LABEL_HEAD[:len(iid)],y_>BENCH_MARK],1).numpy().astype(np.bool)\n",
    "    os.system(\"echo '%s' >> %s\"%(\" \".join(mark[slic].tolist()),fname))\n",
    "    return {\n",
    "        \"ite\":kwargs[\"ite\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.action = test_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Comment load_() if nothing to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "⭐[ep_0_i_1239]\tite\t1236.000: 100%|██████████| 1241/1241 [18:34<00:00,  1.11it/s]\n"
     ]
    }
   ],
   "source": [
    "trainer.train(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(fname,mode=\"r\")\n",
    "fstr = f.read()\n",
    "fstr = fstr.replace(\"\\n\\n\",\"\\n\").replace(\", \",\",\")\n",
    "f.close()\n",
    "f2=open(fname,mode=\"w\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "619704"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f2.write(fstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
